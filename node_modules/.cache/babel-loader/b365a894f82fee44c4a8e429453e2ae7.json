{"ast":null,"code":"// Copyright (c) Microsoft Corporation. All rights reserved.\n// Licensed under the MIT License.\nimport { InferenceSession as InferenceSessionImpl } from './inference-session-impl'; // eslint-disable-next-line @typescript-eslint/naming-convention\n\nexport var InferenceSession = InferenceSessionImpl;","map":{"version":3,"mappings":"AAAA;AACA;AAEA,SAAQA,gBAAgB,IAAIC,oBAA5B,QAAuD,0BAAvD,C,CAoXA;;AACA,OAAO,IAAMD,gBAAgB,GAA4BC,oBAAlD","names":["InferenceSession","InferenceSessionImpl"],"sources":["/Users/nguyentrungchinh/project_25/node_modules/onnxruntime-common/lib/inference-session.ts"],"sourcesContent":["// Copyright (c) Microsoft Corporation. All rights reserved.\r\n// Licensed under the MIT License.\r\n\r\nimport {InferenceSession as InferenceSessionImpl} from './inference-session-impl';\r\nimport {OnnxValue} from './onnx-value';\r\n\r\n/* eslint-disable @typescript-eslint/no-redeclare */\r\n\r\nexport declare namespace InferenceSession {\r\n  //#region input/output types\r\n\r\n  type OnnxValueMapType = {readonly [name: string]: OnnxValue};\r\n  type NullableOnnxValueMapType = {readonly [name: string]: OnnxValue | null};\r\n\r\n  /**\r\n   * A feeds (model inputs) is an object that uses input names as keys and OnnxValue as corresponding values.\r\n   */\r\n  type FeedsType = OnnxValueMapType;\r\n\r\n  /**\r\n   * A fetches (model outputs) could be one of the following:\r\n   *\r\n   * - Omitted. Use model's output names definition.\r\n   * - An array of string indicating the output names.\r\n   * - An object that use output names as keys and OnnxValue or null as corresponding values.\r\n   *\r\n   * @remark\r\n   * different from input argument, in output, OnnxValue is optional. If an OnnxValue is present it will be\r\n   * used as a pre-allocated value by the inference engine; if omitted, inference engine will allocate buffer\r\n   * internally.\r\n   */\r\n  type FetchesType = readonly string[]|NullableOnnxValueMapType;\r\n\r\n  /**\r\n   * A inferencing return type is an object that uses output names as keys and OnnxValue as corresponding values.\r\n   */\r\n  type ReturnType = OnnxValueMapType;\r\n\r\n  //#endregion\r\n\r\n  //#region session options\r\n\r\n  /**\r\n   * A set of configurations for session behavior.\r\n   */\r\n  export interface SessionOptions {\r\n    /**\r\n     * An array of execution provider options.\r\n     *\r\n     * An execution provider option can be a string indicating the name of the execution provider,\r\n     * or an object of corresponding type.\r\n     */\r\n    executionProviders?: readonly ExecutionProviderConfig[];\r\n\r\n    /**\r\n     * The intra OP threads number.\r\n     *\r\n     * This setting is available only in ONNXRuntime (Node.js binding and react-native).\r\n     */\r\n    intraOpNumThreads?: number;\r\n\r\n    /**\r\n     * The inter OP threads number.\r\n     *\r\n     * This setting is available only in ONNXRuntime (Node.js binding and react-native).\r\n     */\r\n    interOpNumThreads?: number;\r\n\r\n    /**\r\n     * The optimization level.\r\n     *\r\n     * This setting is available only in ONNXRuntime (Node.js binding and react-native) or WebAssembly backend\r\n     */\r\n    graphOptimizationLevel?: 'disabled'|'basic'|'extended'|'all';\r\n\r\n    /**\r\n     * Whether enable CPU memory arena.\r\n     *\r\n     * This setting is available only in ONNXRuntime (Node.js binding and react-native) or WebAssembly backend\r\n     */\r\n    enableCpuMemArena?: boolean;\r\n\r\n    /**\r\n     * Whether enable memory pattern.\r\n     *\r\n     * This setting is available only in ONNXRuntime (Node.js binding and react-native) or WebAssembly backend\r\n     */\r\n    enableMemPattern?: boolean;\r\n\r\n    /**\r\n     * Execution mode.\r\n     *\r\n     * This setting is available only in ONNXRuntime (Node.js binding and react-native) or WebAssembly backend\r\n     */\r\n    executionMode?: 'sequential'|'parallel';\r\n\r\n    /**\r\n     * Wether enable profiling.\r\n     *\r\n     * This setting is a placeholder for a future use.\r\n     */\r\n    enableProfiling?: boolean;\r\n\r\n    /**\r\n     * File prefix for profiling.\r\n     *\r\n     * This setting is a placeholder for a future use.\r\n     */\r\n    profileFilePrefix?: string;\r\n\r\n    /**\r\n     * Log ID.\r\n     *\r\n     * This setting is available only in ONNXRuntime (Node.js binding and react-native) or WebAssembly backend\r\n     */\r\n    logId?: string;\r\n\r\n    /**\r\n     * Log severity level. See\r\n     * https://github.com/microsoft/onnxruntime/blob/master/include/onnxruntime/core/common/logging/severity.h\r\n     *\r\n     * This setting is available only in ONNXRuntime (Node.js binding and react-native) or WebAssembly backend\r\n     */\r\n    logSeverityLevel?: 0|1|2|3|4;\r\n\r\n    /**\r\n     * Log verbosity level.\r\n     *\r\n     * This setting is available only in WebAssembly backend. Will support Node.js binding and react-native later\r\n     */\r\n    logVerbosityLevel?: number;\r\n\r\n    /**\r\n     * Store configurations for a session. See\r\n     * https://github.com/microsoft/onnxruntime/blob/master/include/onnxruntime/core/session/\r\n     * onnxruntime_session_options_config_keys.h\r\n     *\r\n     * This setting is available only in WebAssembly backend. Will support Node.js binding and react-native later\r\n     *\r\n     * @example\r\n     * ```js\r\n     * extra: {\r\n     *   session: {\r\n     *     set_denormal_as_zero: \"1\",\r\n     *     disable_prepacking: \"1\"\r\n     *   },\r\n     *   optimization: {\r\n     *     enable_gelu_approximation: \"1\"\r\n     *   }\r\n     * }\r\n     * ```\r\n     */\r\n    extra?: Record<string, unknown>;\r\n  }\r\n\r\n  //#region execution providers\r\n\r\n  // Currently, we have the following backends to support execution providers:\r\n  // Backend Node.js binding: supports 'cpu' and 'cuda'.\r\n  // Backend WebAssembly: supports 'wasm'.\r\n  // Backend ONNX.js: supports 'webgl'.\r\n  interface ExecutionProviderOptionMap {\r\n    cpu: CpuExecutionProviderOption;\r\n    cuda: CudaExecutionProviderOption;\r\n    wasm: WebAssemblyExecutionProviderOption;\r\n    webgl: WebGLExecutionProviderOption;\r\n  }\r\n\r\n  type ExecutionProviderName = keyof ExecutionProviderOptionMap;\r\n  type ExecutionProviderConfig =\r\n      ExecutionProviderOptionMap[ExecutionProviderName]|ExecutionProviderOption|ExecutionProviderName|string;\r\n\r\n  export interface ExecutionProviderOption {\r\n    readonly name: string;\r\n  }\r\n  export interface CpuExecutionProviderOption extends ExecutionProviderOption {\r\n    readonly name: 'cpu';\r\n    useArena?: boolean;\r\n  }\r\n  export interface CudaExecutionProviderOption extends ExecutionProviderOption {\r\n    readonly name: 'cuda';\r\n    deviceId?: number;\r\n  }\r\n  export interface WebAssemblyExecutionProviderOption extends ExecutionProviderOption {\r\n    readonly name: 'wasm';\r\n    // TODO: add flags\r\n  }\r\n  export interface WebGLExecutionProviderOption extends ExecutionProviderOption {\r\n    readonly name: 'webgl';\r\n    // TODO: add flags\r\n  }\r\n  //#endregion\r\n\r\n  //#endregion\r\n\r\n  //#region run options\r\n\r\n  /**\r\n   * A set of configurations for inference run behavior\r\n   */\r\n  export interface RunOptions {\r\n    /**\r\n     * Log severity level. See\r\n     * https://github.com/microsoft/onnxruntime/blob/master/include/onnxruntime/core/common/logging/severity.h\r\n     *\r\n     * This setting is available only in ONNXRuntime (Node.js binding and react-native) or WebAssembly backend\r\n     */\r\n    logSeverityLevel?: 0|1|2|3|4;\r\n\r\n    /**\r\n     * Log verbosity level.\r\n     *\r\n     * This setting is available only in WebAssembly backend. Will support Node.js binding and react-native later\r\n     */\r\n    logVerbosityLevel?: number;\r\n\r\n    /**\r\n     * Terminate all incomplete OrtRun calls as soon as possible if true\r\n     *\r\n     * This setting is available only in WebAssembly backend. Will support Node.js binding and react-native later\r\n     */\r\n    terminate?: boolean;\r\n\r\n    /**\r\n     * A tag for the Run() calls using this\r\n     *\r\n     * This setting is available only in ONNXRuntime (Node.js binding and react-native) or WebAssembly backend\r\n     */\r\n    tag?: string;\r\n\r\n    /**\r\n     * Set a single run configuration entry. See\r\n     * https://github.com/microsoft/onnxruntime/blob/master/include/onnxruntime/core/session/\r\n     * onnxruntime_run_options_config_keys.h\r\n     *\r\n     * This setting is available only in WebAssembly backend. Will support Node.js binding and react-native later\r\n     *\r\n     * @example\r\n     *\r\n     * ```js\r\n     * extra: {\r\n     *   memory: {\r\n     *     enable_memory_arena_shrinkage: \"1\",\r\n     *   }\r\n     * }\r\n     * ```\r\n     */\r\n    extra?: Record<string, unknown>;\r\n  }\r\n\r\n  //#endregion\r\n\r\n  //#region value metadata\r\n\r\n  // eslint-disable-next-line @typescript-eslint/no-empty-interface\r\n  interface ValueMetadata {\r\n    // TBD\r\n  }\r\n\r\n  //#endregion\r\n}\r\n\r\n/**\r\n * Represent a runtime instance of an ONNX model.\r\n */\r\nexport interface InferenceSession {\r\n  //#region run()\r\n\r\n  /**\r\n   * Execute the model asynchronously with the given feeds and options.\r\n   *\r\n   * @param feeds - Representation of the model input. See type description of `InferenceSession.InputType` for detail.\r\n   * @param options - Optional. A set of options that controls the behavior of model inference.\r\n   * @returns A promise that resolves to a map, which uses output names as keys and OnnxValue as corresponding values.\r\n   */\r\n  run(feeds: InferenceSession.FeedsType, options?: InferenceSession.RunOptions): Promise<InferenceSession.ReturnType>;\r\n\r\n  /**\r\n   * Execute the model asynchronously with the given feeds, fetches and options.\r\n   *\r\n   * @param feeds - Representation of the model input. See type description of `InferenceSession.InputType` for detail.\r\n   * @param fetches - Representation of the model output. See type description of `InferenceSession.OutputType` for\r\n   * detail.\r\n   * @param options - Optional. A set of options that controls the behavior of model inference.\r\n   * @returns A promise that resolves to a map, which uses output names as keys and OnnxValue as corresponding values.\r\n   */\r\n  run(feeds: InferenceSession.FeedsType, fetches: InferenceSession.FetchesType,\r\n      options?: InferenceSession.RunOptions): Promise<InferenceSession.ReturnType>;\r\n\r\n  //#endregion\r\n\r\n  //#region profiling\r\n\r\n  /**\r\n   * Start profiling.\r\n   */\r\n  startProfiling(): void;\r\n\r\n  /**\r\n   * End profiling.\r\n   */\r\n  endProfiling(): void;\r\n\r\n  //#endregion\r\n\r\n  //#region metadata\r\n\r\n  /**\r\n   * Get input names of the loaded model.\r\n   */\r\n  readonly inputNames: readonly string[];\r\n\r\n  /**\r\n   * Get output names of the loaded model.\r\n   */\r\n  readonly outputNames: readonly string[];\r\n\r\n  // /**\r\n  //  * Get input metadata of the loaded model.\r\n  //  */\r\n  // readonly inputMetadata: ReadonlyArray<Readonly<InferenceSession.ValueMetadata>>;\r\n\r\n  // /**\r\n  //  * Get output metadata of the loaded model.\r\n  //  */\r\n  // readonly outputMetadata: ReadonlyArray<Readonly<InferenceSession.ValueMetadata>>;\r\n\r\n  //#endregion\r\n}\r\n\r\nexport interface InferenceSessionFactory {\r\n  //#region create()\r\n\r\n  /**\r\n   * Create a new inference session and load model asynchronously from an ONNX model file.\r\n   *\r\n   * @param uri - The URI or file path of the model to load.\r\n   * @param options - specify configuration for creating a new inference session.\r\n   * @returns A promise that resolves to an InferenceSession object.\r\n   */\r\n  create(uri: string, options?: InferenceSession.SessionOptions): Promise<InferenceSession>;\r\n\r\n  /**\r\n   * Create a new inference session and load model asynchronously from an array bufer.\r\n   *\r\n   * @param buffer - An ArrayBuffer representation of an ONNX model.\r\n   * @param options - specify configuration for creating a new inference session.\r\n   * @returns A promise that resolves to an InferenceSession object.\r\n   */\r\n  create(buffer: ArrayBufferLike, options?: InferenceSession.SessionOptions): Promise<InferenceSession>;\r\n\r\n  /**\r\n   * Create a new inference session and load model asynchronously from segment of an array bufer.\r\n   *\r\n   * @param buffer - An ArrayBuffer representation of an ONNX model.\r\n   * @param byteOffset - The beginning of the specified portion of the array buffer.\r\n   * @param byteLength - The length in bytes of the array buffer.\r\n   * @param options - specify configuration for creating a new inference session.\r\n   * @returns A promise that resolves to an InferenceSession object.\r\n   */\r\n  create(buffer: ArrayBufferLike, byteOffset: number, byteLength?: number, options?: InferenceSession.SessionOptions):\r\n      Promise<InferenceSession>;\r\n\r\n  /**\r\n   * Create a new inference session and load model asynchronously from a Uint8Array.\r\n   *\r\n   * @param buffer - A Uint8Array representation of an ONNX model.\r\n   * @param options - specify configuration for creating a new inference session.\r\n   * @returns A promise that resolves to an InferenceSession object.\r\n   */\r\n  create(buffer: Uint8Array, options?: InferenceSession.SessionOptions): Promise<InferenceSession>;\r\n\r\n  //#endregion\r\n}\r\n\r\n// eslint-disable-next-line @typescript-eslint/naming-convention\r\nexport const InferenceSession: InferenceSessionFactory = InferenceSessionImpl;\r\n"]},"metadata":{},"sourceType":"module"}