{"ast":null,"code":"/**\n * This tokenizer is a copy of \n * https://raw.githubusercontent.com/tensorflow/tfjs-models/master/qna/src/bert_tokenizer.ts\n * with minor modifications (Removed all tfjs dependencies)\n * \n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nconst SEPERATOR = '\\u2581';\nexport const UNK_INDEX = 100;\nexport const CLS_INDEX = 101;\nexport const CLS_TOKEN = '[CLS]';\nexport const SEP_INDEX = 102;\nexport const SEP_TOKEN = '[SEP]';\nexport const NFKC_TOKEN = 'NFKC';\nexport const VOCAB_URL = './static/vocab.json';\n/**\n * Class for represent node for token parsing Trie data structure.\n */\n\nclass TrieNode {\n  constructor(key) {\n    this.key = key;\n    this.parent = void 0;\n    this.children = {};\n    this.end = false;\n    this.score = void 0;\n    this.index = void 0;\n  }\n\n  getWord() {\n    const output = [];\n    let node = this;\n\n    while (node != null) {\n      if (node.key != null) {\n        output.unshift(node.key);\n      }\n\n      node = node.parent;\n    }\n\n    return [output, this.score, this.index];\n  }\n\n}\n\nclass Trie {\n  constructor() {\n    this.root = new TrieNode(null);\n  }\n\n  /**\n   * Insert the bert vacabulary word into the trie.\n   * @param word word to be inserted.\n   * @param score word score.\n   * @param index index of word in the bert vocabulary file.\n   */\n  insert(word, score, index) {\n    let node = this.root;\n    const symbols = [];\n\n    for (const symbol of word) {\n      symbols.push(symbol);\n    }\n\n    for (let i = 0; i < symbols.length; i++) {\n      if (node.children[symbols[i]] == null) {\n        node.children[symbols[i]] = new TrieNode(symbols[i]);\n        node.children[symbols[i]].parent = node;\n      }\n\n      node = node.children[symbols[i]];\n\n      if (i === symbols.length - 1) {\n        node.end = true;\n        node.score = score;\n        node.index = index;\n      }\n    }\n  }\n  /**\n   * Find the Trie node for the given token, it will return the first node that\n   * matches the subtoken from the beginning of the token.\n   * @param token string, input string to be searched.\n   */\n\n\n  find(token) {\n    let node = this.root;\n    let iter = 0;\n\n    while (iter < token.length && node != null) {\n      node = node.children[token[iter]];\n      iter++;\n    }\n\n    return node;\n  }\n\n}\n\nfunction isWhitespace(ch) {\n  return /\\s/.test(ch);\n}\n\nfunction isInvalid(ch) {\n  return ch.charCodeAt(0) === 0 || ch.charCodeAt(0) === 0xfffd;\n}\n\nconst punctuations = '[~`!@#$%^&*(){}[];:\"\\'<,.>?/\\\\|-_+=';\n/** To judge whether it's a punctuation. */\n\nfunction isPunctuation(ch) {\n  return punctuations.indexOf(ch) !== -1;\n}\n\n/**\n * Tokenizer for Bert.\n */\nexport class BertTokenizer {\n  constructor() {\n    this.vocab = void 0;\n    this.trie = void 0;\n  }\n\n  /**\n   * Load the vacabulary file and initialize the Trie for lookup.\n   */\n  async load() {\n    this.vocab = await this.loadVocab();\n    this.trie = new Trie(); // Actual tokens start at 999.\n\n    for (let vocabIndex = 999; vocabIndex < this.vocab.length; vocabIndex++) {\n      const word = this.vocab[vocabIndex];\n      this.trie.insert(word, 1, vocabIndex);\n    }\n  }\n\n  async loadVocab() {\n    return fetch(VOCAB_URL).then(d => d.json());\n  }\n\n  processInput(text) {\n    const charOriginalIndex = [];\n    const cleanedText = this.cleanText(text, charOriginalIndex);\n    const origTokens = cleanedText.split(' ');\n    let charCount = 0;\n    const tokens = origTokens.map(token => {\n      token = token.toLowerCase();\n      const tokens = this.runSplitOnPunc(token, charCount, charOriginalIndex);\n      charCount += token.length + 1;\n      return tokens;\n    });\n    let flattenTokens = [];\n\n    for (let index = 0; index < tokens.length; index++) {\n      flattenTokens = flattenTokens.concat(tokens[index]);\n    }\n\n    return flattenTokens;\n  }\n  /* Performs invalid character removal and whitespace cleanup on text. */\n\n\n  cleanText(text, charOriginalIndex) {\n    const stringBuilder = [];\n    let originalCharIndex = 0,\n        newCharIndex = 0;\n\n    for (const ch of text) {\n      // Skip the characters that cannot be used.\n      if (isInvalid(ch)) {\n        originalCharIndex += ch.length;\n        continue;\n      }\n\n      if (isWhitespace(ch)) {\n        if (stringBuilder.length > 0 && stringBuilder[stringBuilder.length - 1] !== ' ') {\n          stringBuilder.push(' ');\n          charOriginalIndex[newCharIndex] = originalCharIndex;\n          originalCharIndex += ch.length;\n        } else {\n          originalCharIndex += ch.length;\n          continue;\n        }\n      } else {\n        stringBuilder.push(ch);\n        charOriginalIndex[newCharIndex] = originalCharIndex;\n        originalCharIndex += ch.length;\n      }\n\n      newCharIndex++;\n    }\n\n    return stringBuilder.join('');\n  }\n  /* Splits punctuation on a piece of text. */\n\n\n  runSplitOnPunc(text, count, charOriginalIndex) {\n    const tokens = [];\n    let startNewWord = true;\n\n    for (const ch of text) {\n      if (isPunctuation(ch)) {\n        tokens.push({\n          text: ch,\n          index: charOriginalIndex[count]\n        });\n        count += ch.length;\n        startNewWord = true;\n      } else {\n        if (startNewWord) {\n          tokens.push({\n            text: '',\n            index: charOriginalIndex[count]\n          });\n          startNewWord = false;\n        }\n\n        tokens[tokens.length - 1].text += ch;\n        count += ch.length;\n      }\n    }\n\n    return tokens;\n  }\n  /**\n   * Generate tokens for the given vocalbuary.\n   * @param text text to be tokenized.\n   */\n\n\n  tokenize(text) {\n    // Source:\n    // https://github.com/google-research/bert/blob/88a817c37f788702a363ff935fd173b6dc6ac0d6/tokenization.py#L311\n    let outputTokens = [];\n    const words = this.processInput(text);\n    words.forEach(word => {\n      if (word.text !== CLS_TOKEN && word.text !== SEP_TOKEN) {\n        word.text = `${SEPERATOR}${word.text.normalize(NFKC_TOKEN)}`;\n      }\n    });\n\n    for (let i = 0; i < words.length; i++) {\n      const chars = [];\n\n      for (const symbol of words[i].text) {\n        chars.push(symbol);\n      }\n\n      let isUnknown = false;\n      let start = 0;\n      const subTokens = [];\n      const charsLength = chars.length;\n\n      while (start < charsLength) {\n        let end = charsLength;\n        let currIndex;\n\n        while (start < end) {\n          const substr = chars.slice(start, end).join('');\n          const match = this.trie.find(substr);\n\n          if (match != null && match.end != null) {\n            currIndex = match.getWord()[2];\n            break;\n          }\n\n          end = end - 1;\n        }\n\n        if (currIndex == null) {\n          isUnknown = true;\n          break;\n        }\n\n        subTokens.push(currIndex);\n        start = end;\n      }\n\n      if (isUnknown) {\n        outputTokens.push(UNK_INDEX);\n      } else {\n        outputTokens = outputTokens.concat(subTokens);\n      }\n    }\n\n    return outputTokens;\n  }\n\n}\nexport async function loadTokenizer() {\n  const tokenizer = new BertTokenizer();\n  await tokenizer.load();\n  return tokenizer;\n}","map":{"version":3,"names":["SEPERATOR","UNK_INDEX","CLS_INDEX","CLS_TOKEN","SEP_INDEX","SEP_TOKEN","NFKC_TOKEN","VOCAB_URL","TrieNode","constructor","key","parent","children","end","score","index","getWord","output","node","unshift","Trie","root","insert","word","symbols","symbol","push","i","length","find","token","iter","isWhitespace","ch","test","isInvalid","charCodeAt","punctuations","isPunctuation","indexOf","BertTokenizer","vocab","trie","load","loadVocab","vocabIndex","fetch","then","d","json","processInput","text","charOriginalIndex","cleanedText","cleanText","origTokens","split","charCount","tokens","map","toLowerCase","runSplitOnPunc","flattenTokens","concat","stringBuilder","originalCharIndex","newCharIndex","join","count","startNewWord","tokenize","outputTokens","words","forEach","normalize","chars","isUnknown","start","subTokens","charsLength","currIndex","substr","slice","match","loadTokenizer","tokenizer"],"sources":["/Users/nguyentrungchinh/project_19/src/bert_tokenizer.ts"],"sourcesContent":["/**\n * This tokenizer is a copy of \n * https://raw.githubusercontent.com/tensorflow/tfjs-models/master/qna/src/bert_tokenizer.ts\n * with minor modifications (Removed all tfjs dependencies)\n * \n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nconst SEPERATOR = '\\u2581';\nexport const UNK_INDEX = 100;\nexport const CLS_INDEX = 101;\nexport const CLS_TOKEN = '[CLS]';\nexport const SEP_INDEX = 102;\nexport const SEP_TOKEN = '[SEP]';\nexport const NFKC_TOKEN = 'NFKC';\nexport const VOCAB_URL = './static/vocab.json';\n\n/**\n * Class for represent node for token parsing Trie data structure.\n */\nclass TrieNode {\n  parent: TrieNode;\n  children: {[key: string]: TrieNode} = {};\n  end = false;\n  score: number;\n  index: number;\n  constructor(private key: string) {}\n\n  getWord(): [string[], number, number] {\n    const output: string[] = [];\n    let node: TrieNode = this;\n\n    while (node != null) {\n      if (node.key != null) {\n        output.unshift(node.key);\n      }\n      node = node.parent;\n    }\n\n    return [output, this.score, this.index];\n  }\n}\n\nclass Trie {\n  private root = new TrieNode(null);\n\n  /**\n   * Insert the bert vacabulary word into the trie.\n   * @param word word to be inserted.\n   * @param score word score.\n   * @param index index of word in the bert vocabulary file.\n   */\n  insert(word: string, score: number, index: number) {\n    let node = this.root;\n\n    const symbols = [];\n    for (const symbol of word) {\n      symbols.push(symbol);\n    }\n\n    for (let i = 0; i < symbols.length; i++) {\n      if (node.children[symbols[i]] == null) {\n        node.children[symbols[i]] = new TrieNode(symbols[i]);\n        node.children[symbols[i]].parent = node;\n      }\n\n      node = node.children[symbols[i]];\n\n      if (i === symbols.length - 1) {\n        node.end = true;\n        node.score = score;\n        node.index = index;\n      }\n    }\n  }\n\n  /**\n   * Find the Trie node for the given token, it will return the first node that\n   * matches the subtoken from the beginning of the token.\n   * @param token string, input string to be searched.\n   */\n  find(token: string): TrieNode {\n    let node = this.root;\n    let iter = 0;\n\n    while (iter < token.length && node != null) {\n      node = node.children[token[iter]];\n      iter++;\n    }\n\n    return node;\n  }\n}\n\nfunction isWhitespace(ch: string): boolean {\n  return /\\s/.test(ch);\n}\n\nfunction isInvalid(ch: string): boolean {\n  return (ch.charCodeAt(0) === 0 || ch.charCodeAt(0) === 0xfffd);\n}\n\nconst punctuations = '[~`!@#$%^&*(){}[];:\"\\'<,.>?/\\\\|-_+=';\n\n/** To judge whether it's a punctuation. */\nfunction isPunctuation(ch: string): boolean {\n  return punctuations.indexOf(ch) !== -1;\n}\n\nexport interface Token {\n  text: string;\n  index: number;\n}\n/**\n * Tokenizer for Bert.\n */\nexport class BertTokenizer {\n  private vocab: string[];\n  private trie: Trie;\n\n  /**\n   * Load the vacabulary file and initialize the Trie for lookup.\n   */\n  async load() {\n    this.vocab = await this.loadVocab();\n\n    this.trie = new Trie();\n    // Actual tokens start at 999.\n    for (let vocabIndex = 999; vocabIndex < this.vocab.length; vocabIndex++) {\n      const word = this.vocab[vocabIndex];\n      this.trie.insert(word, 1, vocabIndex);\n    }\n  }\n\n  private async loadVocab(): Promise<[]> {\n    return fetch(VOCAB_URL).then(d => d.json());\n  }\n\n  processInput(text: string): Token[] {\n    const charOriginalIndex: number[] = [];\n    const cleanedText = this.cleanText(text, charOriginalIndex);\n    const origTokens = cleanedText.split(' ');\n\n    let charCount = 0;\n    const tokens = origTokens.map((token) => {\n      token = token.toLowerCase();\n      const tokens = this.runSplitOnPunc(token, charCount, charOriginalIndex);\n      charCount += token.length + 1;\n      return tokens;\n    });\n\n    let flattenTokens: Token[] = [];\n    for (let index = 0; index < tokens.length; index++) {\n      flattenTokens = flattenTokens.concat(tokens[index]);\n    }\n    return flattenTokens;\n  }\n\n  /* Performs invalid character removal and whitespace cleanup on text. */\n  private cleanText(text: string, charOriginalIndex: number[]): string {\n    const stringBuilder: string[] = [];\n    let originalCharIndex = 0, newCharIndex = 0;\n    for (const ch of text) {\n      // Skip the characters that cannot be used.\n      if (isInvalid(ch)) {\n        originalCharIndex += ch.length;\n        continue;\n      }\n      if (isWhitespace(ch)) {\n        if (stringBuilder.length > 0 &&\n            stringBuilder[stringBuilder.length - 1] !== ' ') {\n          stringBuilder.push(' ');\n          charOriginalIndex[newCharIndex] = originalCharIndex;\n          originalCharIndex += ch.length;\n        } else {\n          originalCharIndex += ch.length;\n          continue;\n        }\n      } else {\n        stringBuilder.push(ch);\n        charOriginalIndex[newCharIndex] = originalCharIndex;\n        originalCharIndex += ch.length;\n      }\n      newCharIndex++;\n    }\n    return stringBuilder.join('');\n  }\n\n  /* Splits punctuation on a piece of text. */\n  private runSplitOnPunc(\n      text: string, count: number,\n      charOriginalIndex: number[]): Token[] {\n    const tokens: Token[] = [];\n    let startNewWord = true;\n    for (const ch of text) {\n      if (isPunctuation(ch)) {\n        tokens.push({text: ch, index: charOriginalIndex[count]});\n        count += ch.length;\n        startNewWord = true;\n      } else {\n        if (startNewWord) {\n          tokens.push({text: '', index: charOriginalIndex[count]});\n          startNewWord = false;\n        }\n        tokens[tokens.length - 1].text += ch;\n        count += ch.length;\n      }\n    }\n    return tokens;\n  }\n\n  /**\n   * Generate tokens for the given vocalbuary.\n   * @param text text to be tokenized.\n   */\n  tokenize(text: string): number[] {\n    // Source:\n    // https://github.com/google-research/bert/blob/88a817c37f788702a363ff935fd173b6dc6ac0d6/tokenization.py#L311\n\n    let outputTokens: number[] = [];\n\n    const words = this.processInput(text);\n    words.forEach(word => {\n      if (word.text !== CLS_TOKEN && word.text !== SEP_TOKEN) {\n        word.text = `${SEPERATOR}${word.text.normalize(NFKC_TOKEN)}`;\n      }\n    });\n\n    for (let i = 0; i < words.length; i++) {\n      const chars = [];\n      for (const symbol of words[i].text) {\n        chars.push(symbol);\n      }\n\n      let isUnknown = false;\n      let start = 0;\n      const subTokens: number[] = [];\n\n      const charsLength = chars.length;\n\n      while (start < charsLength) {\n        let end = charsLength;\n        let currIndex;\n\n        while (start < end) {\n          const substr = chars.slice(start, end).join('');\n\n          const match = this.trie.find(substr);\n          if (match != null && match.end != null) {\n            currIndex = match.getWord()[2];\n            break;\n          }\n\n          end = end - 1;\n        }\n\n        if (currIndex == null) {\n          isUnknown = true;\n          break;\n        }\n\n        subTokens.push(currIndex);\n        start = end;\n      }\n\n      if (isUnknown) {\n        outputTokens.push(UNK_INDEX);\n      } else {\n        outputTokens = outputTokens.concat(subTokens);\n      }\n    }\n\n    return outputTokens;\n  }\n}\n\nexport async function loadTokenizer(): Promise<BertTokenizer> {\n  const tokenizer = new BertTokenizer();\n  await tokenizer.load();\n  return tokenizer;\n}\n"],"mappings":"AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA,MAAMA,SAAS,GAAG,QAAlB;AACA,OAAO,MAAMC,SAAS,GAAG,GAAlB;AACP,OAAO,MAAMC,SAAS,GAAG,GAAlB;AACP,OAAO,MAAMC,SAAS,GAAG,OAAlB;AACP,OAAO,MAAMC,SAAS,GAAG,GAAlB;AACP,OAAO,MAAMC,SAAS,GAAG,OAAlB;AACP,OAAO,MAAMC,UAAU,GAAG,MAAnB;AACP,OAAO,MAAMC,SAAS,GAAG,qBAAlB;AAEP;AACA;AACA;;AACA,MAAMC,QAAN,CAAe;EAMbC,WAAW,CAASC,GAAT,EAAsB;IAAA,KAAbA,GAAa,GAAbA,GAAa;IAAA,KALjCC,MAKiC;IAAA,KAJjCC,QAIiC,GAJK,EAIL;IAAA,KAHjCC,GAGiC,GAH3B,KAG2B;IAAA,KAFjCC,KAEiC;IAAA,KADjCC,KACiC;EAAE;;EAEnCC,OAAO,GAA+B;IACpC,MAAMC,MAAgB,GAAG,EAAzB;IACA,IAAIC,IAAc,GAAG,IAArB;;IAEA,OAAOA,IAAI,IAAI,IAAf,EAAqB;MACnB,IAAIA,IAAI,CAACR,GAAL,IAAY,IAAhB,EAAsB;QACpBO,MAAM,CAACE,OAAP,CAAeD,IAAI,CAACR,GAApB;MACD;;MACDQ,IAAI,GAAGA,IAAI,CAACP,MAAZ;IACD;;IAED,OAAO,CAACM,MAAD,EAAS,KAAKH,KAAd,EAAqB,KAAKC,KAA1B,CAAP;EACD;;AApBY;;AAuBf,MAAMK,IAAN,CAAW;EAAA;IAAA,KACDC,IADC,GACM,IAAIb,QAAJ,CAAa,IAAb,CADN;EAAA;;EAGT;AACF;AACA;AACA;AACA;AACA;EACEc,MAAM,CAACC,IAAD,EAAeT,KAAf,EAA8BC,KAA9B,EAA6C;IACjD,IAAIG,IAAI,GAAG,KAAKG,IAAhB;IAEA,MAAMG,OAAO,GAAG,EAAhB;;IACA,KAAK,MAAMC,MAAX,IAAqBF,IAArB,EAA2B;MACzBC,OAAO,CAACE,IAAR,CAAaD,MAAb;IACD;;IAED,KAAK,IAAIE,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGH,OAAO,CAACI,MAA5B,EAAoCD,CAAC,EAArC,EAAyC;MACvC,IAAIT,IAAI,CAACN,QAAL,CAAcY,OAAO,CAACG,CAAD,CAArB,KAA6B,IAAjC,EAAuC;QACrCT,IAAI,CAACN,QAAL,CAAcY,OAAO,CAACG,CAAD,CAArB,IAA4B,IAAInB,QAAJ,CAAagB,OAAO,CAACG,CAAD,CAApB,CAA5B;QACAT,IAAI,CAACN,QAAL,CAAcY,OAAO,CAACG,CAAD,CAArB,EAA0BhB,MAA1B,GAAmCO,IAAnC;MACD;;MAEDA,IAAI,GAAGA,IAAI,CAACN,QAAL,CAAcY,OAAO,CAACG,CAAD,CAArB,CAAP;;MAEA,IAAIA,CAAC,KAAKH,OAAO,CAACI,MAAR,GAAiB,CAA3B,EAA8B;QAC5BV,IAAI,CAACL,GAAL,GAAW,IAAX;QACAK,IAAI,CAACJ,KAAL,GAAaA,KAAb;QACAI,IAAI,CAACH,KAAL,GAAaA,KAAb;MACD;IACF;EACF;EAED;AACF;AACA;AACA;AACA;;;EACEc,IAAI,CAACC,KAAD,EAA0B;IAC5B,IAAIZ,IAAI,GAAG,KAAKG,IAAhB;IACA,IAAIU,IAAI,GAAG,CAAX;;IAEA,OAAOA,IAAI,GAAGD,KAAK,CAACF,MAAb,IAAuBV,IAAI,IAAI,IAAtC,EAA4C;MAC1CA,IAAI,GAAGA,IAAI,CAACN,QAAL,CAAckB,KAAK,CAACC,IAAD,CAAnB,CAAP;MACAA,IAAI;IACL;;IAED,OAAOb,IAAP;EACD;;AAhDQ;;AAmDX,SAASc,YAAT,CAAsBC,EAAtB,EAA2C;EACzC,OAAO,KAAKC,IAAL,CAAUD,EAAV,CAAP;AACD;;AAED,SAASE,SAAT,CAAmBF,EAAnB,EAAwC;EACtC,OAAQA,EAAE,CAACG,UAAH,CAAc,CAAd,MAAqB,CAArB,IAA0BH,EAAE,CAACG,UAAH,CAAc,CAAd,MAAqB,MAAvD;AACD;;AAED,MAAMC,YAAY,GAAG,qCAArB;AAEA;;AACA,SAASC,aAAT,CAAuBL,EAAvB,EAA4C;EAC1C,OAAOI,YAAY,CAACE,OAAb,CAAqBN,EAArB,MAA6B,CAAC,CAArC;AACD;;AAMD;AACA;AACA;AACA,OAAO,MAAMO,aAAN,CAAoB;EAAA;IAAA,KACjBC,KADiB;IAAA,KAEjBC,IAFiB;EAAA;;EAIzB;AACF;AACA;EACY,MAAJC,IAAI,GAAG;IACX,KAAKF,KAAL,GAAa,MAAM,KAAKG,SAAL,EAAnB;IAEA,KAAKF,IAAL,GAAY,IAAItB,IAAJ,EAAZ,CAHW,CAIX;;IACA,KAAK,IAAIyB,UAAU,GAAG,GAAtB,EAA2BA,UAAU,GAAG,KAAKJ,KAAL,CAAWb,MAAnD,EAA2DiB,UAAU,EAArE,EAAyE;MACvE,MAAMtB,IAAI,GAAG,KAAKkB,KAAL,CAAWI,UAAX,CAAb;MACA,KAAKH,IAAL,CAAUpB,MAAV,CAAiBC,IAAjB,EAAuB,CAAvB,EAA0BsB,UAA1B;IACD;EACF;;EAEsB,MAATD,SAAS,GAAgB;IACrC,OAAOE,KAAK,CAACvC,SAAD,CAAL,CAAiBwC,IAAjB,CAAsBC,CAAC,IAAIA,CAAC,CAACC,IAAF,EAA3B,CAAP;EACD;;EAEDC,YAAY,CAACC,IAAD,EAAwB;IAClC,MAAMC,iBAA2B,GAAG,EAApC;IACA,MAAMC,WAAW,GAAG,KAAKC,SAAL,CAAeH,IAAf,EAAqBC,iBAArB,CAApB;IACA,MAAMG,UAAU,GAAGF,WAAW,CAACG,KAAZ,CAAkB,GAAlB,CAAnB;IAEA,IAAIC,SAAS,GAAG,CAAhB;IACA,MAAMC,MAAM,GAAGH,UAAU,CAACI,GAAX,CAAgB7B,KAAD,IAAW;MACvCA,KAAK,GAAGA,KAAK,CAAC8B,WAAN,EAAR;MACA,MAAMF,MAAM,GAAG,KAAKG,cAAL,CAAoB/B,KAApB,EAA2B2B,SAA3B,EAAsCL,iBAAtC,CAAf;MACAK,SAAS,IAAI3B,KAAK,CAACF,MAAN,GAAe,CAA5B;MACA,OAAO8B,MAAP;IACD,CALc,CAAf;IAOA,IAAII,aAAsB,GAAG,EAA7B;;IACA,KAAK,IAAI/C,KAAK,GAAG,CAAjB,EAAoBA,KAAK,GAAG2C,MAAM,CAAC9B,MAAnC,EAA2Cb,KAAK,EAAhD,EAAoD;MAClD+C,aAAa,GAAGA,aAAa,CAACC,MAAd,CAAqBL,MAAM,CAAC3C,KAAD,CAA3B,CAAhB;IACD;;IACD,OAAO+C,aAAP;EACD;EAED;;;EACQR,SAAS,CAACH,IAAD,EAAeC,iBAAf,EAAoD;IACnE,MAAMY,aAAuB,GAAG,EAAhC;IACA,IAAIC,iBAAiB,GAAG,CAAxB;IAAA,IAA2BC,YAAY,GAAG,CAA1C;;IACA,KAAK,MAAMjC,EAAX,IAAiBkB,IAAjB,EAAuB;MACrB;MACA,IAAIhB,SAAS,CAACF,EAAD,CAAb,EAAmB;QACjBgC,iBAAiB,IAAIhC,EAAE,CAACL,MAAxB;QACA;MACD;;MACD,IAAII,YAAY,CAACC,EAAD,CAAhB,EAAsB;QACpB,IAAI+B,aAAa,CAACpC,MAAd,GAAuB,CAAvB,IACAoC,aAAa,CAACA,aAAa,CAACpC,MAAd,GAAuB,CAAxB,CAAb,KAA4C,GADhD,EACqD;UACnDoC,aAAa,CAACtC,IAAd,CAAmB,GAAnB;UACA0B,iBAAiB,CAACc,YAAD,CAAjB,GAAkCD,iBAAlC;UACAA,iBAAiB,IAAIhC,EAAE,CAACL,MAAxB;QACD,CALD,MAKO;UACLqC,iBAAiB,IAAIhC,EAAE,CAACL,MAAxB;UACA;QACD;MACF,CAVD,MAUO;QACLoC,aAAa,CAACtC,IAAd,CAAmBO,EAAnB;QACAmB,iBAAiB,CAACc,YAAD,CAAjB,GAAkCD,iBAAlC;QACAA,iBAAiB,IAAIhC,EAAE,CAACL,MAAxB;MACD;;MACDsC,YAAY;IACb;;IACD,OAAOF,aAAa,CAACG,IAAd,CAAmB,EAAnB,CAAP;EACD;EAED;;;EACQN,cAAc,CAClBV,IADkB,EACJiB,KADI,EAElBhB,iBAFkB,EAEoB;IACxC,MAAMM,MAAe,GAAG,EAAxB;IACA,IAAIW,YAAY,GAAG,IAAnB;;IACA,KAAK,MAAMpC,EAAX,IAAiBkB,IAAjB,EAAuB;MACrB,IAAIb,aAAa,CAACL,EAAD,CAAjB,EAAuB;QACrByB,MAAM,CAAChC,IAAP,CAAY;UAACyB,IAAI,EAAElB,EAAP;UAAWlB,KAAK,EAAEqC,iBAAiB,CAACgB,KAAD;QAAnC,CAAZ;QACAA,KAAK,IAAInC,EAAE,CAACL,MAAZ;QACAyC,YAAY,GAAG,IAAf;MACD,CAJD,MAIO;QACL,IAAIA,YAAJ,EAAkB;UAChBX,MAAM,CAAChC,IAAP,CAAY;YAACyB,IAAI,EAAE,EAAP;YAAWpC,KAAK,EAAEqC,iBAAiB,CAACgB,KAAD;UAAnC,CAAZ;UACAC,YAAY,GAAG,KAAf;QACD;;QACDX,MAAM,CAACA,MAAM,CAAC9B,MAAP,GAAgB,CAAjB,CAAN,CAA0BuB,IAA1B,IAAkClB,EAAlC;QACAmC,KAAK,IAAInC,EAAE,CAACL,MAAZ;MACD;IACF;;IACD,OAAO8B,MAAP;EACD;EAED;AACF;AACA;AACA;;;EACEY,QAAQ,CAACnB,IAAD,EAAyB;IAC/B;IACA;IAEA,IAAIoB,YAAsB,GAAG,EAA7B;IAEA,MAAMC,KAAK,GAAG,KAAKtB,YAAL,CAAkBC,IAAlB,CAAd;IACAqB,KAAK,CAACC,OAAN,CAAclD,IAAI,IAAI;MACpB,IAAIA,IAAI,CAAC4B,IAAL,KAAchD,SAAd,IAA2BoB,IAAI,CAAC4B,IAAL,KAAc9C,SAA7C,EAAwD;QACtDkB,IAAI,CAAC4B,IAAL,GAAa,GAAEnD,SAAU,GAAEuB,IAAI,CAAC4B,IAAL,CAAUuB,SAAV,CAAoBpE,UAApB,CAAgC,EAA3D;MACD;IACF,CAJD;;IAMA,KAAK,IAAIqB,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG6C,KAAK,CAAC5C,MAA1B,EAAkCD,CAAC,EAAnC,EAAuC;MACrC,MAAMgD,KAAK,GAAG,EAAd;;MACA,KAAK,MAAMlD,MAAX,IAAqB+C,KAAK,CAAC7C,CAAD,CAAL,CAASwB,IAA9B,EAAoC;QAClCwB,KAAK,CAACjD,IAAN,CAAWD,MAAX;MACD;;MAED,IAAImD,SAAS,GAAG,KAAhB;MACA,IAAIC,KAAK,GAAG,CAAZ;MACA,MAAMC,SAAmB,GAAG,EAA5B;MAEA,MAAMC,WAAW,GAAGJ,KAAK,CAAC/C,MAA1B;;MAEA,OAAOiD,KAAK,GAAGE,WAAf,EAA4B;QAC1B,IAAIlE,GAAG,GAAGkE,WAAV;QACA,IAAIC,SAAJ;;QAEA,OAAOH,KAAK,GAAGhE,GAAf,EAAoB;UAClB,MAAMoE,MAAM,GAAGN,KAAK,CAACO,KAAN,CAAYL,KAAZ,EAAmBhE,GAAnB,EAAwBsD,IAAxB,CAA6B,EAA7B,CAAf;UAEA,MAAMgB,KAAK,GAAG,KAAKzC,IAAL,CAAUb,IAAV,CAAeoD,MAAf,CAAd;;UACA,IAAIE,KAAK,IAAI,IAAT,IAAiBA,KAAK,CAACtE,GAAN,IAAa,IAAlC,EAAwC;YACtCmE,SAAS,GAAGG,KAAK,CAACnE,OAAN,GAAgB,CAAhB,CAAZ;YACA;UACD;;UAEDH,GAAG,GAAGA,GAAG,GAAG,CAAZ;QACD;;QAED,IAAImE,SAAS,IAAI,IAAjB,EAAuB;UACrBJ,SAAS,GAAG,IAAZ;UACA;QACD;;QAEDE,SAAS,CAACpD,IAAV,CAAesD,SAAf;QACAH,KAAK,GAAGhE,GAAR;MACD;;MAED,IAAI+D,SAAJ,EAAe;QACbL,YAAY,CAAC7C,IAAb,CAAkBzB,SAAlB;MACD,CAFD,MAEO;QACLsE,YAAY,GAAGA,YAAY,CAACR,MAAb,CAAoBe,SAApB,CAAf;MACD;IACF;;IAED,OAAOP,YAAP;EACD;;AA7JwB;AAgK3B,OAAO,eAAea,aAAf,GAAuD;EAC5D,MAAMC,SAAS,GAAG,IAAI7C,aAAJ,EAAlB;EACA,MAAM6C,SAAS,CAAC1C,IAAV,EAAN;EACA,OAAO0C,SAAP;AACD"},"metadata":{},"sourceType":"module"}