{"ast":null,"code":"import _regeneratorRuntime from\"/Users/nguyentrungchinh/project_20/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";import _asyncToGenerator from\"/Users/nguyentrungchinh/project_20/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";import _createForOfIteratorHelper from\"/Users/nguyentrungchinh/project_20/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";import _classCallCheck from\"/Users/nguyentrungchinh/project_20/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";import _createClass from\"/Users/nguyentrungchinh/project_20/node_modules/@babel/runtime/helpers/esm/createClass.js\";/**\n * This tokenizer is a copy of \n * https://raw.githubusercontent.com/tensorflow/tfjs-models/master/qna/src/bert_tokenizer.ts\n * with minor modifications (Removed all tfjs dependencies)\n * \n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */var SEPERATOR=\"\\u2581\";export var UNK_INDEX=100;export var CLS_INDEX=101;export var CLS_TOKEN='[CLS]';export var SEP_INDEX=102;export var SEP_TOKEN='[SEP]';export var NFKC_TOKEN='NFKC';export var VOCAB_URL='./static/vocab.json';/**\n * Class for represent node for token parsing Trie data structure.\n */var TrieNode=/*#__PURE__*/function(){function TrieNode(key){_classCallCheck(this,TrieNode);this.key=key;this.parent=void 0;this.children={};this.end=false;this.score=void 0;this.index=void 0;}_createClass(TrieNode,[{key:\"getWord\",value:function getWord(){var output=[];var node=this;while(node!=null){if(node.key!=null){output.unshift(node.key);}node=node.parent;}return[output,this.score,this.index];}}]);return TrieNode;}();var Trie=/*#__PURE__*/function(){function Trie(){_classCallCheck(this,Trie);this.root=new TrieNode(null);}_createClass(Trie,[{key:\"insert\",value:/**\n   * Insert the bert vacabulary word into the trie.\n   * @param word word to be inserted.\n   * @param score word score.\n   * @param index index of word in the bert vocabulary file.\n   */function insert(word,score,index){var node=this.root;var symbols=[];var _iterator=_createForOfIteratorHelper(word),_step;try{for(_iterator.s();!(_step=_iterator.n()).done;){var symbol=_step.value;symbols.push(symbol);}}catch(err){_iterator.e(err);}finally{_iterator.f();}for(var i=0;i<symbols.length;i++){if(node.children[symbols[i]]==null){node.children[symbols[i]]=new TrieNode(symbols[i]);node.children[symbols[i]].parent=node;}node=node.children[symbols[i]];if(i===symbols.length-1){node.end=true;node.score=score;node.index=index;}}}/**\n   * Find the Trie node for the given token, it will return the first node that\n   * matches the subtoken from the beginning of the token.\n   * @param token string, input string to be searched.\n   */},{key:\"find\",value:function find(token){var node=this.root;var iter=0;while(iter<token.length&&node!=null){node=node.children[token[iter]];iter++;}return node;}}]);return Trie;}();function isWhitespace(ch){return /\\s/.test(ch);}function isInvalid(ch){return ch.charCodeAt(0)===0||ch.charCodeAt(0)===0xfffd;}var punctuations='[~`!@#$%^&*(){}[];:\"\\'<,.>?/\\\\|-_+=';/** To judge whether it's a punctuation. */function isPunctuation(ch){return punctuations.indexOf(ch)!==-1;}/**\n * Tokenizer for Bert.\n */export var BertTokenizer=/*#__PURE__*/function(){function BertTokenizer(){_classCallCheck(this,BertTokenizer);this.vocab=void 0;this.trie=void 0;}_createClass(BertTokenizer,[{key:\"load\",value:/**\n   * Load the vacabulary file and initialize the Trie for lookup.\n   */function(){var _load=_asyncToGenerator(/*#__PURE__*/_regeneratorRuntime().mark(function _callee(){var vocabIndex,word;return _regeneratorRuntime().wrap(function _callee$(_context){while(1){switch(_context.prev=_context.next){case 0:_context.next=2;return this.loadVocab();case 2:this.vocab=_context.sent;this.trie=new Trie();// Actual tokens start at 999.\nfor(vocabIndex=999;vocabIndex<this.vocab.length;vocabIndex++){word=this.vocab[vocabIndex];this.trie.insert(word,1,vocabIndex);}case 5:case\"end\":return _context.stop();}}},_callee,this);}));function load(){return _load.apply(this,arguments);}return load;}()},{key:\"loadVocab\",value:function(){var _loadVocab=_asyncToGenerator(/*#__PURE__*/_regeneratorRuntime().mark(function _callee2(){return _regeneratorRuntime().wrap(function _callee2$(_context2){while(1){switch(_context2.prev=_context2.next){case 0:return _context2.abrupt(\"return\",fetch(VOCAB_URL).then(function(d){return d.json();}));case 1:case\"end\":return _context2.stop();}}},_callee2);}));function loadVocab(){return _loadVocab.apply(this,arguments);}return loadVocab;}()},{key:\"processInput\",value:function processInput(text){var _this=this;var charOriginalIndex=[];var cleanedText=this.cleanText(text,charOriginalIndex);var origTokens=cleanedText.split(' ');var charCount=0;var tokens=origTokens.map(function(token){token=token.toLowerCase();var tokens=_this.runSplitOnPunc(token,charCount,charOriginalIndex);charCount+=token.length+1;return tokens;});var flattenTokens=[];for(var index=0;index<tokens.length;index++){flattenTokens=flattenTokens.concat(tokens[index]);}return flattenTokens;}/* Performs invalid character removal and whitespace cleanup on text. */},{key:\"cleanText\",value:function cleanText(text,charOriginalIndex){var stringBuilder=[];var originalCharIndex=0,newCharIndex=0;var _iterator2=_createForOfIteratorHelper(text),_step2;try{for(_iterator2.s();!(_step2=_iterator2.n()).done;){var ch=_step2.value;// Skip the characters that cannot be used.\nif(isInvalid(ch)){originalCharIndex+=ch.length;continue;}if(isWhitespace(ch)){if(stringBuilder.length>0&&stringBuilder[stringBuilder.length-1]!==' '){stringBuilder.push(' ');charOriginalIndex[newCharIndex]=originalCharIndex;originalCharIndex+=ch.length;}else{originalCharIndex+=ch.length;continue;}}else{stringBuilder.push(ch);charOriginalIndex[newCharIndex]=originalCharIndex;originalCharIndex+=ch.length;}newCharIndex++;}}catch(err){_iterator2.e(err);}finally{_iterator2.f();}return stringBuilder.join('');}/* Splits punctuation on a piece of text. */},{key:\"runSplitOnPunc\",value:function runSplitOnPunc(text,count,charOriginalIndex){var tokens=[];var startNewWord=true;var _iterator3=_createForOfIteratorHelper(text),_step3;try{for(_iterator3.s();!(_step3=_iterator3.n()).done;){var ch=_step3.value;if(isPunctuation(ch)){tokens.push({text:ch,index:charOriginalIndex[count]});count+=ch.length;startNewWord=true;}else{if(startNewWord){tokens.push({text:'',index:charOriginalIndex[count]});startNewWord=false;}tokens[tokens.length-1].text+=ch;count+=ch.length;}}}catch(err){_iterator3.e(err);}finally{_iterator3.f();}return tokens;}/**\n   * Generate tokens for the given vocalbuary.\n   * @param text text to be tokenized.\n   */},{key:\"tokenize\",value:function tokenize(text){// Source:\n// https://github.com/google-research/bert/blob/88a817c37f788702a363ff935fd173b6dc6ac0d6/tokenization.py#L311\nvar outputTokens=[];var words=this.processInput(text);words.forEach(function(word){if(word.text!==CLS_TOKEN&&word.text!==SEP_TOKEN){word.text=\"\".concat(SEPERATOR).concat(word.text.normalize(NFKC_TOKEN));}});for(var i=0;i<words.length;i++){var chars=[];var _iterator4=_createForOfIteratorHelper(words[i].text),_step4;try{for(_iterator4.s();!(_step4=_iterator4.n()).done;){var symbol=_step4.value;chars.push(symbol);}}catch(err){_iterator4.e(err);}finally{_iterator4.f();}var isUnknown=false;var start=0;var subTokens=[];var charsLength=chars.length;while(start<charsLength){var end=charsLength;var currIndex=void 0;while(start<end){var substr=chars.slice(start,end).join('');var match=this.trie.find(substr);if(match!=null&&match.end!=null){currIndex=match.getWord()[2];break;}end=end-1;}if(currIndex==null){isUnknown=true;break;}subTokens.push(currIndex);start=end;}if(isUnknown){outputTokens.push(UNK_INDEX);}else{outputTokens=outputTokens.concat(subTokens);}}return outputTokens;}}]);return BertTokenizer;}();export function loadTokenizer(){return _loadTokenizer.apply(this,arguments);}function _loadTokenizer(){_loadTokenizer=_asyncToGenerator(/*#__PURE__*/_regeneratorRuntime().mark(function _callee3(){var tokenizer;return _regeneratorRuntime().wrap(function _callee3$(_context3){while(1){switch(_context3.prev=_context3.next){case 0:tokenizer=new BertTokenizer();_context3.next=3;return tokenizer.load();case 3:return _context3.abrupt(\"return\",tokenizer);case 4:case\"end\":return _context3.stop();}}},_callee3);}));return _loadTokenizer.apply(this,arguments);}","map":{"version":3,"names":["SEPERATOR","UNK_INDEX","CLS_INDEX","CLS_TOKEN","SEP_INDEX","SEP_TOKEN","NFKC_TOKEN","VOCAB_URL","TrieNode","key","parent","children","end","score","index","output","node","unshift","Trie","root","word","symbols","symbol","push","i","length","token","iter","isWhitespace","ch","test","isInvalid","charCodeAt","punctuations","isPunctuation","indexOf","BertTokenizer","vocab","trie","loadVocab","vocabIndex","insert","fetch","then","d","json","text","charOriginalIndex","cleanedText","cleanText","origTokens","split","charCount","tokens","map","toLowerCase","runSplitOnPunc","flattenTokens","concat","stringBuilder","originalCharIndex","newCharIndex","join","count","startNewWord","outputTokens","words","processInput","forEach","normalize","chars","isUnknown","start","subTokens","charsLength","currIndex","substr","slice","match","find","getWord","loadTokenizer","tokenizer","load"],"sources":["/Users/nguyentrungchinh/project_20/src/bert_tokenizer.ts"],"sourcesContent":["/**\n * This tokenizer is a copy of \n * https://raw.githubusercontent.com/tensorflow/tfjs-models/master/qna/src/bert_tokenizer.ts\n * with minor modifications (Removed all tfjs dependencies)\n * \n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nconst SEPERATOR = '\\u2581';\nexport const UNK_INDEX = 100;\nexport const CLS_INDEX = 101;\nexport const CLS_TOKEN = '[CLS]';\nexport const SEP_INDEX = 102;\nexport const SEP_TOKEN = '[SEP]';\nexport const NFKC_TOKEN = 'NFKC';\nexport const VOCAB_URL = './static/vocab.json';\n\n/**\n * Class for represent node for token parsing Trie data structure.\n */\nclass TrieNode {\n  parent: TrieNode;\n  children: {[key: string]: TrieNode} = {};\n  end = false;\n  score: number;\n  index: number;\n  constructor(private key: string) {}\n\n  getWord(): [string[], number, number] {\n    const output: string[] = [];\n    let node: TrieNode = this;\n\n    while (node != null) {\n      if (node.key != null) {\n        output.unshift(node.key);\n      }\n      node = node.parent;\n    }\n\n    return [output, this.score, this.index];\n  }\n}\n\nclass Trie {\n  private root = new TrieNode(null);\n\n  /**\n   * Insert the bert vacabulary word into the trie.\n   * @param word word to be inserted.\n   * @param score word score.\n   * @param index index of word in the bert vocabulary file.\n   */\n  insert(word: string, score: number, index: number) {\n    let node = this.root;\n\n    const symbols = [];\n    for (const symbol of word) {\n      symbols.push(symbol);\n    }\n\n    for (let i = 0; i < symbols.length; i++) {\n      if (node.children[symbols[i]] == null) {\n        node.children[symbols[i]] = new TrieNode(symbols[i]);\n        node.children[symbols[i]].parent = node;\n      }\n\n      node = node.children[symbols[i]];\n\n      if (i === symbols.length - 1) {\n        node.end = true;\n        node.score = score;\n        node.index = index;\n      }\n    }\n  }\n\n  /**\n   * Find the Trie node for the given token, it will return the first node that\n   * matches the subtoken from the beginning of the token.\n   * @param token string, input string to be searched.\n   */\n  find(token: string): TrieNode {\n    let node = this.root;\n    let iter = 0;\n\n    while (iter < token.length && node != null) {\n      node = node.children[token[iter]];\n      iter++;\n    }\n\n    return node;\n  }\n}\n\nfunction isWhitespace(ch: string): boolean {\n  return /\\s/.test(ch);\n}\n\nfunction isInvalid(ch: string): boolean {\n  return (ch.charCodeAt(0) === 0 || ch.charCodeAt(0) === 0xfffd);\n}\n\nconst punctuations = '[~`!@#$%^&*(){}[];:\"\\'<,.>?/\\\\|-_+=';\n\n/** To judge whether it's a punctuation. */\nfunction isPunctuation(ch: string): boolean {\n  return punctuations.indexOf(ch) !== -1;\n}\n\nexport interface Token {\n  text: string;\n  index: number;\n}\n/**\n * Tokenizer for Bert.\n */\nexport class BertTokenizer {\n  private vocab: string[];\n  private trie: Trie;\n\n  /**\n   * Load the vacabulary file and initialize the Trie for lookup.\n   */\n  async load() {\n    this.vocab = await this.loadVocab();\n\n    this.trie = new Trie();\n    // Actual tokens start at 999.\n    for (let vocabIndex = 999; vocabIndex < this.vocab.length; vocabIndex++) {\n      const word = this.vocab[vocabIndex];\n      this.trie.insert(word, 1, vocabIndex);\n    }\n  }\n\n  private async loadVocab(): Promise<[]> {\n    return fetch(VOCAB_URL).then(d => d.json());\n  }\n\n  processInput(text: string): Token[] {\n    const charOriginalIndex: number[] = [];\n    const cleanedText = this.cleanText(text, charOriginalIndex);\n    const origTokens = cleanedText.split(' ');\n\n    let charCount = 0;\n    const tokens = origTokens.map((token) => {\n      token = token.toLowerCase();\n      const tokens = this.runSplitOnPunc(token, charCount, charOriginalIndex);\n      charCount += token.length + 1;\n      return tokens;\n    });\n\n    let flattenTokens: Token[] = [];\n    for (let index = 0; index < tokens.length; index++) {\n      flattenTokens = flattenTokens.concat(tokens[index]);\n    }\n    return flattenTokens;\n  }\n\n  /* Performs invalid character removal and whitespace cleanup on text. */\n  private cleanText(text: string, charOriginalIndex: number[]): string {\n    const stringBuilder: string[] = [];\n    let originalCharIndex = 0, newCharIndex = 0;\n    for (const ch of text) {\n      // Skip the characters that cannot be used.\n      if (isInvalid(ch)) {\n        originalCharIndex += ch.length;\n        continue;\n      }\n      if (isWhitespace(ch)) {\n        if (stringBuilder.length > 0 &&\n            stringBuilder[stringBuilder.length - 1] !== ' ') {\n          stringBuilder.push(' ');\n          charOriginalIndex[newCharIndex] = originalCharIndex;\n          originalCharIndex += ch.length;\n        } else {\n          originalCharIndex += ch.length;\n          continue;\n        }\n      } else {\n        stringBuilder.push(ch);\n        charOriginalIndex[newCharIndex] = originalCharIndex;\n        originalCharIndex += ch.length;\n      }\n      newCharIndex++;\n    }\n    return stringBuilder.join('');\n  }\n\n  /* Splits punctuation on a piece of text. */\n  private runSplitOnPunc(\n      text: string, count: number,\n      charOriginalIndex: number[]): Token[] {\n    const tokens: Token[] = [];\n    let startNewWord = true;\n    for (const ch of text) {\n      if (isPunctuation(ch)) {\n        tokens.push({text: ch, index: charOriginalIndex[count]});\n        count += ch.length;\n        startNewWord = true;\n      } else {\n        if (startNewWord) {\n          tokens.push({text: '', index: charOriginalIndex[count]});\n          startNewWord = false;\n        }\n        tokens[tokens.length - 1].text += ch;\n        count += ch.length;\n      }\n    }\n    return tokens;\n  }\n\n  /**\n   * Generate tokens for the given vocalbuary.\n   * @param text text to be tokenized.\n   */\n  tokenize(text: string): number[] {\n    // Source:\n    // https://github.com/google-research/bert/blob/88a817c37f788702a363ff935fd173b6dc6ac0d6/tokenization.py#L311\n\n    let outputTokens: number[] = [];\n\n    const words = this.processInput(text);\n    words.forEach(word => {\n      if (word.text !== CLS_TOKEN && word.text !== SEP_TOKEN) {\n        word.text = `${SEPERATOR}${word.text.normalize(NFKC_TOKEN)}`;\n      }\n    });\n\n    for (let i = 0; i < words.length; i++) {\n      const chars = [];\n      for (const symbol of words[i].text) {\n        chars.push(symbol);\n      }\n\n      let isUnknown = false;\n      let start = 0;\n      const subTokens: number[] = [];\n\n      const charsLength = chars.length;\n\n      while (start < charsLength) {\n        let end = charsLength;\n        let currIndex;\n\n        while (start < end) {\n          const substr = chars.slice(start, end).join('');\n\n          const match = this.trie.find(substr);\n          if (match != null && match.end != null) {\n            currIndex = match.getWord()[2];\n            break;\n          }\n\n          end = end - 1;\n        }\n\n        if (currIndex == null) {\n          isUnknown = true;\n          break;\n        }\n\n        subTokens.push(currIndex);\n        start = end;\n      }\n\n      if (isUnknown) {\n        outputTokens.push(UNK_INDEX);\n      } else {\n        outputTokens = outputTokens.concat(subTokens);\n      }\n    }\n\n    return outputTokens;\n  }\n}\n\nexport async function loadTokenizer(): Promise<BertTokenizer> {\n  const tokenizer = new BertTokenizer();\n  await tokenizer.load();\n  return tokenizer;\n}\n"],"mappings":"8nBAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAEA,GAAMA,UAAS,CAAG,QAAlB,CACA,MAAO,IAAMC,UAAS,CAAG,GAAlB,CACP,MAAO,IAAMC,UAAS,CAAG,GAAlB,CACP,MAAO,IAAMC,UAAS,CAAG,OAAlB,CACP,MAAO,IAAMC,UAAS,CAAG,GAAlB,CACP,MAAO,IAAMC,UAAS,CAAG,OAAlB,CACP,MAAO,IAAMC,WAAU,CAAG,MAAnB,CACP,MAAO,IAAMC,UAAS,CAAG,qBAAlB,CAEP;AACA;AACA,G,GACMC,S,yBAMJ,kBAAoBC,GAApB,CAAiC,qCAAbA,GAAa,CAAbA,GAAa,MALjCC,MAKiC,aAJjCC,QAIiC,CAJK,EAIL,MAHjCC,GAGiC,CAH3B,KAG2B,MAFjCC,KAEiC,aADjCC,KACiC,QAAE,C,4CAEnC,kBAAsC,CACpC,GAAMC,OAAgB,CAAG,EAAzB,CACA,GAAIC,KAAc,CAAG,IAArB,CAEA,MAAOA,IAAI,EAAI,IAAf,CAAqB,CACnB,GAAIA,IAAI,CAACP,GAAL,EAAY,IAAhB,CAAsB,CACpBM,MAAM,CAACE,OAAP,CAAeD,IAAI,CAACP,GAApB,EACD,CACDO,IAAI,CAAGA,IAAI,CAACN,MAAZ,CACD,CAED,MAAO,CAACK,MAAD,CAAS,KAAKF,KAAd,CAAqB,KAAKC,KAA1B,CAAP,CACD,C,2BAGGI,K,yEACIC,I,CAAO,GAAIX,SAAJ,CAAa,IAAb,C,yCAEf;AACF;AACA;AACA;AACA;AACA,KACE,gBAAOY,IAAP,CAAqBP,KAArB,CAAoCC,KAApC,CAAmD,CACjD,GAAIE,KAAI,CAAG,KAAKG,IAAhB,CAEA,GAAME,QAAO,CAAG,EAAhB,CAHiD,yCAI5BD,IAJ4B,YAIjD,+CAA2B,IAAhBE,OAAgB,aACzBD,OAAO,CAACE,IAAR,CAAaD,MAAb,EACD,CANgD,qDAQjD,IAAK,GAAIE,EAAC,CAAG,CAAb,CAAgBA,CAAC,CAAGH,OAAO,CAACI,MAA5B,CAAoCD,CAAC,EAArC,CAAyC,CACvC,GAAIR,IAAI,CAACL,QAAL,CAAcU,OAAO,CAACG,CAAD,CAArB,GAA6B,IAAjC,CAAuC,CACrCR,IAAI,CAACL,QAAL,CAAcU,OAAO,CAACG,CAAD,CAArB,EAA4B,GAAIhB,SAAJ,CAAaa,OAAO,CAACG,CAAD,CAApB,CAA5B,CACAR,IAAI,CAACL,QAAL,CAAcU,OAAO,CAACG,CAAD,CAArB,EAA0Bd,MAA1B,CAAmCM,IAAnC,CACD,CAEDA,IAAI,CAAGA,IAAI,CAACL,QAAL,CAAcU,OAAO,CAACG,CAAD,CAArB,CAAP,CAEA,GAAIA,CAAC,GAAKH,OAAO,CAACI,MAAR,CAAiB,CAA3B,CAA8B,CAC5BT,IAAI,CAACJ,GAAL,CAAW,IAAX,CACAI,IAAI,CAACH,KAAL,CAAaA,KAAb,CACAG,IAAI,CAACF,KAAL,CAAaA,KAAb,CACD,CACF,CACF,CAED;AACF;AACA;AACA;AACA,K,oBACE,cAAKY,KAAL,CAA8B,CAC5B,GAAIV,KAAI,CAAG,KAAKG,IAAhB,CACA,GAAIQ,KAAI,CAAG,CAAX,CAEA,MAAOA,IAAI,CAAGD,KAAK,CAACD,MAAb,EAAuBT,IAAI,EAAI,IAAtC,CAA4C,CAC1CA,IAAI,CAAGA,IAAI,CAACL,QAAL,CAAce,KAAK,CAACC,IAAD,CAAnB,CAAP,CACAA,IAAI,GACL,CAED,MAAOX,KAAP,CACD,C,oBAGH,QAASY,aAAT,CAAsBC,EAAtB,CAA2C,CACzC,MAAO,MAAKC,IAAL,CAAUD,EAAV,CAAP,CACD,CAED,QAASE,UAAT,CAAmBF,EAAnB,CAAwC,CACtC,MAAQA,GAAE,CAACG,UAAH,CAAc,CAAd,IAAqB,CAArB,EAA0BH,EAAE,CAACG,UAAH,CAAc,CAAd,IAAqB,MAAvD,CACD,CAED,GAAMC,aAAY,CAAG,qCAArB,CAEA,2CACA,QAASC,cAAT,CAAuBL,EAAvB,CAA4C,CAC1C,MAAOI,aAAY,CAACE,OAAb,CAAqBN,EAArB,IAA6B,CAAC,CAArC,CACD,CAMD;AACA;AACA,GACA,UAAaO,cAAb,2FACUC,KADV,aAEUC,IAFV,uDAIE;AACF;AACA,KANA,+EAOE,+KACqB,MAAKC,SAAL,EADrB,QACE,KAAKF,KADP,eAGE,KAAKC,IAAL,CAAY,GAAIpB,KAAJ,EAAZ,CACA;AACA,IAASsB,UAAT,CAAsB,GAAtB,CAA2BA,UAAU,CAAG,KAAKH,KAAL,CAAWZ,MAAnD,CAA2De,UAAU,EAArE,CAAyE,CACjEpB,IADiE,CAC1D,KAAKiB,KAAL,CAAWG,UAAX,CAD0D,CAEvE,KAAKF,IAAL,CAAUG,MAAV,CAAiBrB,IAAjB,CAAuB,CAAvB,CAA0BoB,UAA1B,EACD,CARH,2DAPF,mLAkBE,2KACSE,KAAK,CAACnC,SAAD,CAAL,CAAiBoC,IAAjB,CAAsB,SAAAC,CAAC,QAAIA,EAAC,CAACC,IAAF,EAAJ,EAAvB,CADT,0DAlBF,iHAsBE,sBAAaC,IAAb,CAAoC,gBAClC,GAAMC,kBAA2B,CAAG,EAApC,CACA,GAAMC,YAAW,CAAG,KAAKC,SAAL,CAAeH,IAAf,CAAqBC,iBAArB,CAApB,CACA,GAAMG,WAAU,CAAGF,WAAW,CAACG,KAAZ,CAAkB,GAAlB,CAAnB,CAEA,GAAIC,UAAS,CAAG,CAAhB,CACA,GAAMC,OAAM,CAAGH,UAAU,CAACI,GAAX,CAAe,SAAC5B,KAAD,CAAW,CACvCA,KAAK,CAAGA,KAAK,CAAC6B,WAAN,EAAR,CACA,GAAMF,OAAM,CAAG,KAAI,CAACG,cAAL,CAAoB9B,KAApB,CAA2B0B,SAA3B,CAAsCL,iBAAtC,CAAf,CACAK,SAAS,EAAI1B,KAAK,CAACD,MAAN,CAAe,CAA5B,CACA,MAAO4B,OAAP,CACD,CALc,CAAf,CAOA,GAAII,cAAsB,CAAG,EAA7B,CACA,IAAK,GAAI3C,MAAK,CAAG,CAAjB,CAAoBA,KAAK,CAAGuC,MAAM,CAAC5B,MAAnC,CAA2CX,KAAK,EAAhD,CAAoD,CAClD2C,aAAa,CAAGA,aAAa,CAACC,MAAd,CAAqBL,MAAM,CAACvC,KAAD,CAA3B,CAAhB,CACD,CACD,MAAO2C,cAAP,CACD,CAED,wEA1CF,yBA2CE,mBAAkBX,IAAlB,CAAgCC,iBAAhC,CAAqE,CACnE,GAAMY,cAAuB,CAAG,EAAhC,CACA,GAAIC,kBAAiB,CAAG,CAAxB,CAA2BC,YAAY,CAAG,CAA1C,CAFmE,0CAGlDf,IAHkD,aAGnE,kDAAuB,IAAZjB,GAAY,cACrB;AACA,GAAIE,SAAS,CAACF,EAAD,CAAb,CAAmB,CACjB+B,iBAAiB,EAAI/B,EAAE,CAACJ,MAAxB,CACA,SACD,CACD,GAAIG,YAAY,CAACC,EAAD,CAAhB,CAAsB,CACpB,GAAI8B,aAAa,CAAClC,MAAd,CAAuB,CAAvB,EACAkC,aAAa,CAACA,aAAa,CAAClC,MAAd,CAAuB,CAAxB,CAAb,GAA4C,GADhD,CACqD,CACnDkC,aAAa,CAACpC,IAAd,CAAmB,GAAnB,EACAwB,iBAAiB,CAACc,YAAD,CAAjB,CAAkCD,iBAAlC,CACAA,iBAAiB,EAAI/B,EAAE,CAACJ,MAAxB,CACD,CALD,IAKO,CACLmC,iBAAiB,EAAI/B,EAAE,CAACJ,MAAxB,CACA,SACD,CACF,CAVD,IAUO,CACLkC,aAAa,CAACpC,IAAd,CAAmBM,EAAnB,EACAkB,iBAAiB,CAACc,YAAD,CAAjB,CAAkCD,iBAAlC,CACAA,iBAAiB,EAAI/B,EAAE,CAACJ,MAAxB,CACD,CACDoC,YAAY,GACb,CAzBkE,uDA0BnE,MAAOF,cAAa,CAACG,IAAd,CAAmB,EAAnB,CAAP,CACD,CAED,4CAxEF,8BAyEE,wBACIhB,IADJ,CACkBiB,KADlB,CAEIhB,iBAFJ,CAE0C,CACxC,GAAMM,OAAe,CAAG,EAAxB,CACA,GAAIW,aAAY,CAAG,IAAnB,CAFwC,0CAGvBlB,IAHuB,aAGxC,kDAAuB,IAAZjB,GAAY,cACrB,GAAIK,aAAa,CAACL,EAAD,CAAjB,CAAuB,CACrBwB,MAAM,CAAC9B,IAAP,CAAY,CAACuB,IAAI,CAAEjB,EAAP,CAAWf,KAAK,CAAEiC,iBAAiB,CAACgB,KAAD,CAAnC,CAAZ,EACAA,KAAK,EAAIlC,EAAE,CAACJ,MAAZ,CACAuC,YAAY,CAAG,IAAf,CACD,CAJD,IAIO,CACL,GAAIA,YAAJ,CAAkB,CAChBX,MAAM,CAAC9B,IAAP,CAAY,CAACuB,IAAI,CAAE,EAAP,CAAWhC,KAAK,CAAEiC,iBAAiB,CAACgB,KAAD,CAAnC,CAAZ,EACAC,YAAY,CAAG,KAAf,CACD,CACDX,MAAM,CAACA,MAAM,CAAC5B,MAAP,CAAgB,CAAjB,CAAN,CAA0BqB,IAA1B,EAAkCjB,EAAlC,CACAkC,KAAK,EAAIlC,EAAE,CAACJ,MAAZ,CACD,CACF,CAhBuC,uDAiBxC,MAAO4B,OAAP,CACD,CAED;AACF;AACA;AACA,KAlGA,wBAmGE,kBAASP,IAAT,CAAiC,CAC/B;AACA;AAEA,GAAImB,aAAsB,CAAG,EAA7B,CAEA,GAAMC,MAAK,CAAG,KAAKC,YAAL,CAAkBrB,IAAlB,CAAd,CACAoB,KAAK,CAACE,OAAN,CAAc,SAAAhD,IAAI,CAAI,CACpB,GAAIA,IAAI,CAAC0B,IAAL,GAAc3C,SAAd,EAA2BiB,IAAI,CAAC0B,IAAL,GAAczC,SAA7C,CAAwD,CACtDe,IAAI,CAAC0B,IAAL,WAAe9C,SAAf,SAA2BoB,IAAI,CAAC0B,IAAL,CAAUuB,SAAV,CAAoB/D,UAApB,CAA3B,EACD,CACF,CAJD,EAMA,IAAK,GAAIkB,EAAC,CAAG,CAAb,CAAgBA,CAAC,CAAG0C,KAAK,CAACzC,MAA1B,CAAkCD,CAAC,EAAnC,CAAuC,CACrC,GAAM8C,MAAK,CAAG,EAAd,CADqC,0CAEhBJ,KAAK,CAAC1C,CAAD,CAAL,CAASsB,IAFO,aAErC,kDAAoC,IAAzBxB,OAAyB,cAClCgD,KAAK,CAAC/C,IAAN,CAAWD,MAAX,EACD,CAJoC,uDAMrC,GAAIiD,UAAS,CAAG,KAAhB,CACA,GAAIC,MAAK,CAAG,CAAZ,CACA,GAAMC,UAAmB,CAAG,EAA5B,CAEA,GAAMC,YAAW,CAAGJ,KAAK,CAAC7C,MAA1B,CAEA,MAAO+C,KAAK,CAAGE,WAAf,CAA4B,CAC1B,GAAI9D,IAAG,CAAG8D,WAAV,CACA,GAAIC,UAAS,OAAb,CAEA,MAAOH,KAAK,CAAG5D,GAAf,CAAoB,CAClB,GAAMgE,OAAM,CAAGN,KAAK,CAACO,KAAN,CAAYL,KAAZ,CAAmB5D,GAAnB,EAAwBkD,IAAxB,CAA6B,EAA7B,CAAf,CAEA,GAAMgB,MAAK,CAAG,KAAKxC,IAAL,CAAUyC,IAAV,CAAeH,MAAf,CAAd,CACA,GAAIE,KAAK,EAAI,IAAT,EAAiBA,KAAK,CAAClE,GAAN,EAAa,IAAlC,CAAwC,CACtC+D,SAAS,CAAGG,KAAK,CAACE,OAAN,GAAgB,CAAhB,CAAZ,CACA,MACD,CAEDpE,GAAG,CAAGA,GAAG,CAAG,CAAZ,CACD,CAED,GAAI+D,SAAS,EAAI,IAAjB,CAAuB,CACrBJ,SAAS,CAAG,IAAZ,CACA,MACD,CAEDE,SAAS,CAAClD,IAAV,CAAeoD,SAAf,EACAH,KAAK,CAAG5D,GAAR,CACD,CAED,GAAI2D,SAAJ,CAAe,CACbN,YAAY,CAAC1C,IAAb,CAAkBtB,SAAlB,EACD,CAFD,IAEO,CACLgE,YAAY,CAAGA,YAAY,CAACP,MAAb,CAAoBe,SAApB,CAAf,CACD,CACF,CAED,MAAOR,aAAP,CACD,CA7JH,6BAgKA,eAAsBgB,cAAtB,gD,mGAAO,wJACCC,SADD,CACa,GAAI9C,cAAJ,EADb,wBAEC8C,UAAS,CAACC,IAAV,EAFD,yCAGED,SAHF,0D"},"metadata":{},"sourceType":"module"}